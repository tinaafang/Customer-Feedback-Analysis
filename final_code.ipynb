{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11qWSvOLC0w4"
      },
      "source": [
        "# **Overview**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckAu7mPmDDGE",
        "outputId": "cbcb59ff-a739-4a83-cac3-22dc94025465"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2cFOxVhaVktq"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/Data Set_csv.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrXqo7z5Vk99",
        "outputId": "22b004eb-bb3c-4344-916c-39d1aafd6f7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(9176, 7)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Easter Eggs**"
      ],
      "metadata": {
        "id": "4Mc33g2ySqKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai langchain\n",
        "!pip install typing_extensions\n",
        "!pip install python-dotenv\n",
        "!pip install openai\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAil_o8yTXKX",
        "outputId": "909f4dbc-cc1b-4986-a993-bc637d9fbc26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.10.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.1.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.26.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.14)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.24)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.6.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.33)\n",
            "Requirement already satisfied: langchain-community<0.1,>=0.0.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.17)\n",
            "Requirement already satisfied: langchain-core<0.2,>=0.1.16 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.18)\n",
            "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.85)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.23.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.16->langchain) (23.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3108, in _dep_map\n",
            "    return self.__dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2901, in __getattr__\n",
            "    raise AttributeError(attr)\n",
            "AttributeError: _DistInfoDistribution__dep_map\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 169, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/req_command.py\", line 242, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 441, in run\n",
            "    conflicts = self._determine_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/commands/install.py\", line 572, in _determine_conflicts\n",
            "    return check_install_conflicts(to_install)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 101, in check_install_conflicts\n",
            "    package_set, _ = create_package_set_from_installed()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/operations/check.py\", line 42, in create_package_set_from_installed\n",
            "    dependencies = list(dist.iter_dependencies())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/metadata/pkg_resources.py\", line 216, in iter_dependencies\n",
            "    return self._dist.requires(extras)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 2821, in requires\n",
            "    dm = self._dep_map\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3110, in _dep_map\n",
            "    self.__dep_map = self._compute_dependencies()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in _compute_dependencies\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3132, in <listcomp>\n",
            "    dm[s_extra] = [r for r in reqs_for_extra(extra) if r not in common]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/pkg_resources/__init__.py\", line 3124, in reqs_for_extra\n",
            "    if not req.marker or req.marker.evaluate({'extra': extra}):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 300, in evaluate\n",
            "    current_environment = default_environment()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 258, in default_environment\n",
            "    iver = format_full_version(sys.implementation.version)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_vendor/packaging/markers.py\", line 252, in format_full_version\n",
            "    if kind != \"final\":\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/pip3\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/main.py\", line 79, in main\n",
            "    return command.main(cmd_args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
            "    return self._main(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
            "    return run(options, args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pip/_internal/cli/base_command.py\", line 206, in exc_logging_wrapper\n",
            "    logger.critical(\"Operation cancelled by user\")\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1523, in critical\n",
            "    if self.isEnabledFor(CRITICAL):\n",
            "  File \"/usr/lib/python3.10/logging/__init__.py\", line 1724, in isEnabledFor\n",
            "    def isEnabledFor(self, level):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (4.9.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_dotenv()\n",
        "\n",
        "#client = OpenAI(base_url=\"http://localhost:4321/v1\", api_key=\"not-needed\")\n",
        "client = OpenAI()\n",
        "\n",
        "#df = pd.read_excel('data/Winter_2024_Scotia_DSD_Data_Set.xlsx')\n",
        "\n",
        "testing_split = [\n",
        "    (1, True),\n",
        "]\n",
        "\n",
        "\n",
        "def format_prompt(comment: str) -> str:\n",
        "    prompt = f\"\"\"\n",
        "    You are acting as review comment analyst for the Scotiabank Mobile Banking Application.\n",
        "    Your job is to identify review comments that are external and irrelvant to the application.\n",
        "\n",
        "    Examples of irrelevant comments:\n",
        "    Lots of errors.\n",
        "    Amazingly easy. I love the prompts while taking the photos and the auto shoot feature. Well done!\n",
        "    I have been given excellent service, and they provide kind consideration for users.\n",
        "    Worst bank on the planet. Liars, cheats, and theifs, they sell your info to scammers, they get frauded more then any other bank, do your research..\n",
        "\n",
        "    Examples of relevant comments:\n",
        "    Works well.\n",
        "    Pleasant.\n",
        "\n",
        "    Please analyze this comment to see if its relevant, and ONLY output TRUE if its relevant and FALSE if its irrelevant or if you are unsure:\n",
        "\n",
        "    {comment}\n",
        "    \"\"\"\n",
        "    return prompt\n",
        "\n",
        "\n",
        "def llm_is_relevant(comment: str) -> bool:\n",
        "    completion = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"You are acting as review comment analyst for the Scotiabank Mobile Banking Application.\"},\n",
        "            {\"role\": \"user\", \"content\": format_prompt(comment)}\n",
        "        ],\n",
        "        temperature=0,\n",
        "    )\n",
        "    completion_result = completion.choices[0].message.content.lower()\n",
        "    if 'true' in completion_result:\n",
        "        return True\n",
        "    elif 'false' in completion_result:\n",
        "        return False\n",
        "\n",
        "    return False\n",
        "\n",
        "\n",
        "for review_id, is_relevant in testing_split:\n",
        "    comment = df.iloc[review_id]['Review']\n",
        "    print(comment)\n",
        "    print(review_id, llm_is_relevant(comment))"
      ],
      "metadata": {
        "id": "Fjp8QqKySpuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_relevant(review):\n",
        "    \"\"\"\n",
        "    Determine if a review is relevant to the Scotiabank Mobile Banking Application.\n",
        "\n",
        "    :param review: A string containing the review text.\n",
        "    :return: True if the review is relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    irrelevant_keywords = [\"photo\", \"auto shoot\", \"hotel\", \"breakfast\", \"food options\", \"homie\"]\n",
        "\n",
        "    # Lowercase the review for case-insensitive matching\n",
        "    review_lower = review.lower()\n",
        "\n",
        "    # Check if any of the irrelevant keywords are in the review\n",
        "    for keyword in irrelevant_keywords:\n",
        "        if keyword in review_lower:\n",
        "            return False\n",
        "\n",
        "    # If none of the irrelevant keywords are found, the review is relevant\n",
        "    return True\n",
        "\n",
        "\n",
        "# Apply the function to the 'Review' column\n",
        "df['Relevance'] = df['Review'].apply(is_relevant)\n",
        "\n",
        "# Display the first few rows of the dataframe with the new 'Relevance' column\n",
        "df.head()\n",
        "\n",
        "# Updating the relevance status for comments like 5, 6, 7, 10, and 14\n",
        "# First, find the indices of these comments in the dataframe\n",
        "indices_to_update = [50, 62, 110, 157, 224]\n",
        "\n",
        "# Update the 'Relevance' column for these indices\n",
        "for index in indices_to_update:\n",
        "    if index in df.index:\n",
        "        df.at[index, 'Relevance'] = True\n",
        "\n",
        "# Display the updated comments to verify the changes\n",
        "df.loc[indices_to_update][['Review', 'Relevance']]\n",
        "\n",
        "\n",
        "# Refining the algorithm to correctly identify relevant comments, especially those related to specific app features like cheque depositing\n",
        "def refined_is_relevant(review):\n",
        "    \"\"\"\n",
        "    Refine the relevance determination for reviews, focusing on specific app features and avoiding false negatives.\n",
        "\n",
        "    :param review: A string containing the review text.\n",
        "    :return: True if the review is relevant, False otherwise.\n",
        "    \"\"\"\n",
        "    # List of keywords that are clearly irrelevant (not related to banking or the app)\n",
        "    clearly_irrelevant_keywords = [\"hotel\", \"breakfast\", \"food options\", \"homie\", \"staying\", \"stay at this hotel\",\n",
        "                                   \"restaurant\", \"server was rude\", \"rooms smelled\", \"bathrooms were clean\",\n",
        "                                   \"historical feel\", \"road trip\", \"hotel was clean\"]\n",
        "\n",
        "    # Lowercase the review for case-insensitive matching\n",
        "    review_lower = review.lower()\n",
        "\n",
        "    # Check if any of the clearly irrelevant keywords are in the review\n",
        "    for keyword in clearly_irrelevant_keywords:\n",
        "        if keyword in review_lower:\n",
        "            return False\n",
        "\n",
        "    # If none of the clearly irrelevant keywords are found, the review is considered relevant\n",
        "    return True\n",
        "\n",
        "\n",
        "# Apply the refined function to the 'Review' column\n",
        "df['Relevance'] = df['Review'].apply(refined_is_relevant)\n",
        "\n",
        "# Updating the relevance of previously identified borderline cases\n",
        "borderline_indices = [431, 477]\n",
        "for index in borderline_indices:\n",
        "    if index in df.index:\n",
        "        df.at[index, 'Relevance'] = True\n",
        "\n",
        "# Display the updated comments to verify the changes\n",
        "df.loc[borderline_indices][['Review', 'Relevance']]"
      ],
      "metadata": {
        "id": "huyenE_7U24L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "941fIXQEDDPr"
      },
      "outputs": [],
      "source": [
        "#df = pd.read_csv('/content/Updated_Comments_Classification.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9u7xKROxDDSg"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH1s2OfTHr6z"
      },
      "outputs": [],
      "source": [
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE9dsa01QAk8"
      },
      "outputs": [],
      "source": [
        "df_new = df[df['Review'].str.split().str.len() >= 3]\n",
        "print(df_new.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uVqzUOMQDth"
      },
      "outputs": [],
      "source": [
        "df_new.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0g9XHVSSK_e"
      },
      "outputs": [],
      "source": [
        "df_new = df_new[df_new['Relevant'] == True]\n",
        "print(df_new.shape)\n",
        "df_new = df_new.drop('Relevant', axis = 1)\n",
        "print(df_new.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Desired** **Features**"
      ],
      "metadata": {
        "id": "dYnj9FpUvY1i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We select the review with more than 50 likes"
      ],
      "metadata": {
        "id": "5g1d4aoo4Jy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_ft = df_new\n",
        "\n",
        "df_f = df_ft.sort_values(by=['Review_Likes'], ascending=False)"
      ],
      "metadata": {
        "id": "-VHGI2GKvYSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_ft = df_ft[df_ft['Review_Likes'] >= 50]\n",
        "\n",
        "df_ft.sort_values(by=['Review_Likes'], ascending=False)\n",
        "\n",
        "df_ft.sort_values(by=['Review_Likes'], ascending=False).to_csv('/content/desired_features.csv')"
      ],
      "metadata": {
        "id": "tGIJCq1YwzU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e_eA3lbFdfY"
      },
      "source": [
        "# **cleaning**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxmOwqYRepvi"
      },
      "outputs": [],
      "source": [
        "df_new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kOSVItWFht2"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "my_stop_words = {'app','scotia','scotiabank','good','bad','sucks','suck','awesome',\n",
        "                 'great','worst','ok','easy','easy use','best','excellent','terrible','nice',\n",
        "                 'efficient','jerk','trash','love','bank','money',\n",
        "                 'big','straightforward','kinda','simple','complicated','enjoy','hate',\n",
        "                 'userfriendly','user friendly','dope','banks','scotiabanks','use','time','thank',\n",
        "                 'amazing','loving','cool','wow','thanks','notch','work'\n",
        "                 'easier','convenient','banking','like','top','really','confusing',\n",
        "                 'simple','complex','canada','canadian','easier','life','improve',\n",
        "                 'horrible','planet','waste','trash','rubbish','garbage','better',\n",
        "                 'fantastic','awesome','secure','find'}\n",
        "stop_words = stop_words | my_stop_words\n",
        "\n",
        "\n",
        "emoji_pattern = re.compile(\"[\"\n",
        "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "\n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    # remove emoji\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    #tokens = word_tokenize(text)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "\n",
        "    return ' '.join(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBm2PKaIMNhf"
      },
      "outputs": [],
      "source": [
        "df_new[\"Review\"] = df_new['Review'].apply(lambda x:preprocess(x))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = df_new[df_new['Review'] != '']"
      ],
      "metadata": {
        "id": "uIDu5VfZwhRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEkh2A-9T7OH"
      },
      "outputs": [],
      "source": [
        "df_new.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHKYMtpfTCLQ"
      },
      "source": [
        "# **Sentiment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob23MZWHe6rC"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import torch\n",
        "from scipy.special import softmax\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import RobertaForSequenceClassification\n",
        "\n",
        "tokenizer_robeta = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "model_roberta = RobertaForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXhK8KpcfFLk"
      },
      "outputs": [],
      "source": [
        "# example\n",
        "encoded_text_ro = tokenizer_robeta('cant transfer money account either mobile app access web browser \\\n",
        "                                    mobile also sign scotia app cant log sign two step password',\n",
        "                                   return_tensors='pt')\n",
        "output_ro = model_roberta(**encoded_text_ro)\n",
        "scores_ro = output_ro.logits[0].detach().numpy()\n",
        "scores_ro = softmax(scores_ro)\n",
        "scores_ro_dict = {\n",
        "      'roberta_neg' : scores_ro[0],\n",
        "      'roberta_neu' : scores_ro[1],\n",
        "      'roberta_pos' : scores_ro[2]\n",
        "}\n",
        "print(scores_ro_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LtQdzRxrgdGc"
      },
      "outputs": [],
      "source": [
        "def sentiment_roberta(text):\n",
        "  encoded_text_ro = tokenizer_robeta(text, return_tensors='pt')\n",
        "  output_ro = model_roberta(**encoded_text_ro)\n",
        "  scores_ro = output_ro.logits[0].detach().numpy()\n",
        "  scores_ro = softmax(scores_ro)\n",
        "  scores_ro_dict = {\n",
        "      'roberta_neg' : scores_ro[0],\n",
        "      'roberta_neu' : scores_ro[1],\n",
        "      'roberta_pos' : scores_ro[2]\n",
        "  }\n",
        "  return scores_ro_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTgOf9ShhFSj"
      },
      "outputs": [],
      "source": [
        "df_new[\"index\"] = np.arange(0, len(df_new))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FvrTTIdEggmv"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "result_roberta = {}\n",
        "for i, row in tqdm(df_new.iterrows(), total=len(df_new)):\n",
        "    try:\n",
        "        text = row['Review']\n",
        "        myid = row['index']\n",
        "        roberta_scores = sentiment_roberta(text)\n",
        "        result_roberta[myid] = roberta_scores\n",
        "    except RuntimeError:\n",
        "        print(f'Broke for id {myid}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLjwZP5yTlHw"
      },
      "outputs": [],
      "source": [
        "df_new.index = np.arange(0, len(df_new))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lT5Y8t8nJzW"
      },
      "outputs": [],
      "source": [
        "roberta_results_df = pd.DataFrame(result_roberta).T\n",
        "roberta_results_df = roberta_results_df.reset_index().rename(columns={'index': 'Id'})\n",
        "roberta_results_df = roberta_results_df.merge(df_new[[\"Review_ID\",\"Date\", \"Rating\",\"Review_Likes\", \"Review\", \"index\"]], how = \"left\", left_on = \"Id\", right_on = \"index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YqDxzrqLoFel"
      },
      "outputs": [],
      "source": [
        "roberta_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtRQNV0zocus"
      },
      "outputs": [],
      "source": [
        "max_sent_array = [0] * len(roberta_results_df)\n",
        "negative_array = [False] * len(roberta_results_df)\n",
        "\n",
        "for i in range(len(roberta_results_df)):\n",
        "  if (roberta_results_df.loc[i, \"roberta_neg\"] >= roberta_results_df.loc[i, \"roberta_neu\"]) and (roberta_results_df.loc[i, \"roberta_neg\"] >= roberta_results_df.loc[i, \"roberta_pos\"]):\n",
        "    max_sent_array[i] = 'neg'\n",
        "  elif (roberta_results_df.loc[i, \"roberta_neu\"] >= roberta_results_df.loc[i, \"roberta_neg\"]) and (roberta_results_df.loc[i, \"roberta_neu\"] >= roberta_results_df.loc[i, \"roberta_pos\"]):\n",
        "    max_sent_array[i] = 'neu'\n",
        "  else:\n",
        "    max_sent_array[i] = 'pos'\n",
        "  if roberta_results_df.loc[i, \"roberta_neg\"] > 0.7:\n",
        "    negative_array[i] = True\n",
        "\n",
        "roberta_results_df.loc[:, \"Sentiment\"] = max_sent_array\n",
        "roberta_results_df.loc[:, \"Negative\"] = negative_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCh93MD0TCWj"
      },
      "outputs": [],
      "source": [
        "roberta_results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lg1n2teeslYJ"
      },
      "source": [
        "# **Pain Point**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bhea93kqyNv"
      },
      "outputs": [],
      "source": [
        "df_angry = roberta_results_df[roberta_results_df['Negative'] == True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmc1wPbesgL8"
      },
      "outputs": [],
      "source": [
        "df_angry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUDKp0UPsonJ"
      },
      "outputs": [],
      "source": [
        "seed_topic_list_2 = [['two-step verification',\n",
        "  '2sv',\n",
        "  'two step',\n",
        "  '2-step',\n",
        "  'verification',\n",
        "  'two',\n",
        "  '2fa',\n",
        "  'step',\n",
        "  'twostep',\n",
        "  'sign',],\n",
        " ['app performance',\n",
        "  'performance',\n",
        "  'slow',\n",
        "  'fast',\n",
        "  'faster',\n",
        "  'slower',\n",
        "  'laggy',\n",
        "  'lag',\n",
        "  'speed',\n",
        "  'responsive',\n",
        "  'unresponsive',\n",
        "  'delay'],\n",
        " ['accessibility',\n",
        "  'not accessible',\n",
        "  'disability',\n",
        "  'vision'],\n",
        " ['appointment',\n",
        "  'booking',\n",
        "  'book',\n",
        "  'request',\n",
        "  'advisor'],\n",
        " ['biometric',\n",
        "  'fingerprint',\n",
        "  'face id',\n",
        "  'finger',\n",
        "  'recognition'],\n",
        " ['budget',\n",
        "  'budgeting',\n",
        "  'track',\n",
        "  'tracking',\n",
        "  'spend'\n",
        "  'spending',\n",
        "  'income',\n",
        "  'graph'],\n",
        " ['chat',\n",
        "  'support chat',\n",
        "  'customer chat',\n",
        "  'chat box',\n",
        "  'chatbox',\n",
        "  'chat bot',\n",
        "  'talk',\n",
        "  'chatbot',\n",
        "  'live',\n",
        "  'person',\n",
        "  'customer service',\n",
        "  'poor service',\n",
        "  'hotline',\n",
        "  'hot line'],\n",
        " ['mobile deposit',\n",
        "  'cheque',\n",
        "  'deposit'],\n",
        " ['credit score',\n",
        "  'score'],\n",
        " ['email money transfer',\n",
        "  'emt',\n",
        "  'e-transfer',\n",
        "  'transfer',\n",
        "  'email',\n",
        "  'interac'],\n",
        " ['error',\n",
        "  'issue',\n",
        "  'problem',\n",
        "  'glitch',\n",
        "  'fault',\n",
        "  'flaw',\n",
        "  'crash',\n",
        "  'glitch',\n",
        "  'bug',\n",
        "  'buggy',\n",
        "  'bad design',\n",
        "  'incorrect',\n",
        "  'ui',\n",
        "  'ux',\n",
        "  'doesnt work',\n",
        "  'navigate',\n",
        "  'navigation',\n",
        "  'functionality',\n",
        "  'ui/ux',\n",
        "  'photo',\n",
        "  'picture',\n",
        "  'capture',\n",
        "  'unexpected',\n",
        "  'unintended',\n",
        "  'crash'],\n",
        " ['fee',\n",
        "  'charge',\n",
        "  'cost',\n",
        "  'service fee',\n",
        "  'charge',\n",
        "  'expensive',\n",
        "  'costly'],\n",
        " ['alert',\n",
        "  'notification',\n",
        "  'info alert',\n",
        "  'warning',\n",
        "  'warn',\n",
        "  'notify'],\n",
        " ['international',\n",
        "  'oversea',\n",
        "  'overseas transfer',\n",
        "  'foreign',\n",
        "  'foreign transfer',\n",
        "  'international',\n",
        "  'transferring',\n",
        "  'internationally',\n",
        "  'transfer',\n",
        "  'cross border',\n",
        "  'border',\n",
        "  'western union',\n",
        "  'international money transfer'],\n",
        " ['investment',\n",
        "  'invest',\n",
        "  'stock',\n",
        "  'portfolio',\n",
        "  'gic',\n",
        "  'guaranteed certificate',\n",
        "  'rrsp',\n",
        "  'registered retirement saving plan',\n",
        "  'eft',\n",
        "  'interest rate',\n",
        "  'rate',\n",
        "  'fund',\n",
        "  'tfsa'],\n",
        " ['login',\n",
        "  'logout',\n",
        "  'sign in',\n",
        "  'sign out',\n",
        "  'log in',\n",
        "  'log out',\n",
        "  'logging'],\n",
        " ['quick balance',\n",
        "  'fast balance',\n",
        "  'balance view',\n",
        "  'balance',\n",
        "  'quick view'],\n",
        " ['new card',\n",
        "  'request card',\n",
        "  'request new card',\n",
        "  'replacement',\n",
        "  'replacement card',\n",
        "  'lost',\n",
        "  'stolen'],\n",
        " ['reward',\n",
        "  'points',\n",
        "  'loyalty',\n",
        "  'scene',\n",
        "  'scene point',\n",
        "  'point',\n",
        "  'shop',\n",
        "  'redeem',\n",
        "  'bonus'],\n",
        " ['save statement',\n",
        "  'share statement',\n",
        "  'statement',\n",
        "  'save',\n",
        "  'share',\n",
        "  'transaction',\n",
        "  'saving',\n",
        "  'sharing',\n",
        "  'history']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OZwBlLaUuTsJ"
      },
      "outputs": [],
      "source": [
        "pain_review = df_angry['Review']\n",
        "\n",
        "pain_review = pain_review.tolist()\n",
        "pain_review = [str(pain) for pain in pain_review]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDUTz2qcZwEX"
      },
      "outputs": [],
      "source": [
        "!pip install bertopic\n",
        "\n",
        "from bertopic import BERTopic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKnnEiPq5Hbs"
      },
      "outputs": [],
      "source": [
        "!pip install hdbscan\n",
        "import umap as umap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "14UgK5O2wTyb"
      },
      "outputs": [],
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdbscan_model_pain = HDBSCAN(min_cluster_size=100, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
        "\n",
        "\n",
        "umap_model_pain = umap.UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8o8c0IrtXIP"
      },
      "outputs": [],
      "source": [
        "pain_topic = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  #umap_model=umap_model_pain,\n",
        "  #hdbscan_model=hdbscan_model_pain,\n",
        "  #vectorizer_model=vectorizer_model,\n",
        "\n",
        "  seed_topic_list=seed_topic_list_2\n",
        ")\n",
        "\n",
        "topics_pain, probs_pain = pain_topic.fit_transform(pain_review)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pain_topic.get_topic_info()"
      ],
      "metadata": {
        "id": "hm2uQihN6r3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdtM2vJdvIe6"
      },
      "outputs": [],
      "source": [
        "pain_topic.get_topic_info().to_csv(\"/content/painpoint.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poNlHSi0UU7R"
      },
      "source": [
        "## **Popularity with** **BERT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4V761PMCYW9"
      },
      "outputs": [],
      "source": [
        "topic = pd.read_csv('/content/Data_Dictionary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvfmItCbDxz0"
      },
      "outputs": [],
      "source": [
        "def preprocess_topics(text):\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "\n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "\n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stop words\n",
        "    tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    tokens = [lmtzr.lemmatize(token) for token in tokens]\n",
        "\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psS9_6AbCZUy"
      },
      "outputs": [],
      "source": [
        "topic['Description'] = topic['Description'].apply(lambda x:preprocess_topics(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gE7wVJxlChbW"
      },
      "outputs": [],
      "source": [
        "topic_words_array = topic['Description'].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-giUjDjc4dw"
      },
      "outputs": [],
      "source": [
        "seed_topic_list = [\n",
        "    [\"two-step verification\", \"2sv\", \"two step\", \"2-step\", \"verification\"],  # 2SV\n",
        "    [\"app performance\", \"performance\", \"slow\", \"crash\", \"bug\"],  # Application Performance\n",
        "    [\"accessibility\", \"accessible\", \"access\"],  # Accessibility\n",
        "    [\"appointment\", \"booking\", \"book\"],  # Appointment Booking\n",
        "    [\"biometric\", \"fingerprint\", \"face id\", \"login\"],  # Biometric Login\n",
        "    [\"budget\", \"budgeting\"],  # Budgeting\n",
        "    [\"chat\", \"support chat\", \"customer chat\"],  # Chat\n",
        "    [\"cheque deposit\", \"deposit cheque\", \"check deposit\", \"mobile deposit\"],  # Cheque Deposit\n",
        "    [\"credit score\", \"credit\"],  # Credit Score\n",
        "    [\"email money transfer\", \"emt\", \"e-transfer\", \"transfer\"],  # Email Money Transfer\n",
        "    [\"error\", \"issue\", \"problem\", \"glitch\", \"fault\"],  # Errors\n",
        "    [\"fee\", \"charge\", \"cost\"],  # Fee\n",
        "    [\"alert\", \"notification\", \"info alert\"],  # Info Alerts\n",
        "    [\"international money\", \"overseas transfer\", \"foreign transfer\"],  # International Money Movement\n",
        "    [\"investment\", \"invest\", \"stock\", \"portfolio\"],  # Investments\n",
        "    [\"login\", \"logout\", \"sign in\", \"sign out\", \"log in\", \"log out\"],  # Login and Logout Issues\n",
        "    [\"quick balance\", \"fast balance\", \"balance view\"],  # Quick Balance\n",
        "    [\"new card\", \"request card\", \"card issue\"],  # Request New Card\n",
        "    [\"reward\", \"points\", \"loyalty\"],  # Rewards\n",
        "    [\"save statement\", \"share statement\", \"statement\"]  # Save and Share Statements\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpQ52j1jCmKs"
      },
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "  seed_topic_list[i].extend(x for x in topic_words_array[i] if x not in seed_topic_list[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QbtLqc2Afngp"
      },
      "outputs": [],
      "source": [
        "docs = df_new['Review']\n",
        "\n",
        "docs = docs.tolist()\n",
        "docs = [str(doc) for doc in docs]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = umap.UMAP(n_neighbors=10, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n"
      ],
      "metadata": {
        "id": "N4PP6UIX7eaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPH--E0c6KLr"
      },
      "outputs": [],
      "source": [
        "topic_model = BERTopic(seed_topic_list=seed_topic_list,umap_model=umap_model)\n",
        "topics, probs = topic_model.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model.get_topic_info()"
      ],
      "metadata": {
        "id": "UqUrwVzz7QuA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYSfTADSRUX2"
      },
      "outputs": [],
      "source": [
        "topic_model.get_topic_info().to_csv(\"/content/bert_topics.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCPRuQzAcHx_"
      },
      "outputs": [],
      "source": [
        "# the model used for labelling\n",
        "topic_model_2 = BERTopic(seed_topic_list=seed_topic_list_2,umap_model=umap_model)\n",
        "topics2, probs2 = topic_model_2.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic_model_2.get_topic_info()"
      ],
      "metadata": {
        "id": "ZmWDGq5cL8rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vW2ac3w6hx03"
      },
      "outputs": [],
      "source": [
        "topic_model_2.get_topic_info().to_csv(\"/content/bert_topics_2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQsnjgRQhm5e"
      },
      "outputs": [],
      "source": [
        "topic_model.get_document_info(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wxzgUBN4peY"
      },
      "outputs": [],
      "source": [
        "from hdbscan import HDBSCAN\n",
        "\n",
        "hdbscan_model = HDBSCAN(min_cluster_size=10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
        "\n",
        "\n",
        "umap_model = umap.UMAP(n_neighbors=100, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seed_topic_list_3 = [['two-step verification',\n",
        "  '2sv',\n",
        "  'two step',\n",
        "  '2-step',\n",
        "  'verification',\n",
        "  'two',\n",
        "  '2fa',\n",
        "  'step',\n",
        "  'twostep',\n",
        "  'sign',],\n",
        " ['app performance',\n",
        "  'performance',\n",
        "  'slow',\n",
        "  'fast',\n",
        "  'faster',\n",
        "  'slower',\n",
        "  'laggy',\n",
        "  'lag',\n",
        "  'speed',\n",
        "  'responsive',\n",
        "  'unresponsive',\n",
        "  'delay'],\n",
        " ['accessibility',\n",
        "  'accessible',\n",
        "  'design',\n",
        "  'not accessible',\n",
        "  'disability',\n",
        "  'vision'],\n",
        " ['appointment',\n",
        "  'booking',\n",
        "  'book',\n",
        "  'request',\n",
        "  'advisor'],\n",
        " ['biometric',\n",
        "  'fingerprint',\n",
        "  'face id',\n",
        "  'finger',\n",
        "  'recognition'],\n",
        " ['budget',\n",
        "  'budgeting',\n",
        "  'track',\n",
        "  'tracking',\n",
        "  'spend'\n",
        "  'spending',\n",
        "  'income',\n",
        "  'graph'],\n",
        " ['chat',\n",
        "  'support chat',\n",
        "  'customer chat',\n",
        "  'chat box',\n",
        "  'chatbox',\n",
        "  'chat bot',\n",
        "  'talk',\n",
        "  'chatbot',\n",
        "  'live',\n",
        "  'person',\n",
        "  'customer service',\n",
        "  'poor service',\n",
        "  'hotline',\n",
        "  'helpline',\n",
        "  'online',\n",
        "  'online customer',\n",
        "  'hot line'],\n",
        " ['mobile deposit',\n",
        "  'cheque',\n",
        "  'deposit',\n",
        "  'cheque_deposit',\n",
        "  'deposit cheque',\n",
        "  'capture',\n",
        "  'photo'],\n",
        " ['credit score',\n",
        "  'score'],\n",
        " ['email money transfer',\n",
        "  'emt',\n",
        "  'e-transfer',\n",
        "  'transfer',\n",
        "  'email',\n",
        "  'interac'],\n",
        " ['error',\n",
        "  'issue',\n",
        "  'problem',\n",
        "  'glitch',\n",
        "  'fault',\n",
        "  'flaw',\n",
        "  'glitch',\n",
        "  'bug',\n",
        "  'buggy',\n",
        "  'bad design',\n",
        "  'incorrect',\n",
        "  'ui',\n",
        "  'ux',\n",
        "  'doesnt work',\n",
        "  'navigate',\n",
        "  'navigation',\n",
        "  'functionality',\n",
        "  'ui/ux',\n",
        "  'photo',\n",
        "  'picture',\n",
        "  'capture',\n",
        "  'unexpected',\n",
        "  'unintended',\n",
        "  'crash'],\n",
        " ['fee',\n",
        "  'charge',\n",
        "  'cost',\n",
        "  'service fee',\n",
        "  'charge',\n",
        "  'expensive',\n",
        "  'costly'],\n",
        " ['alert',\n",
        "  'notification',\n",
        "  'info alert',\n",
        "  'warning',\n",
        "  'warn',\n",
        "  'notify'],\n",
        " ['international',\n",
        "  'oversea',\n",
        "  'overseas transfer',\n",
        "  'foreign',\n",
        "  'foreign transfer',\n",
        "  'international',\n",
        "  'transferring',\n",
        "  'internationally',\n",
        "  'transfer',\n",
        "  'cross border',\n",
        "  'border',\n",
        "  'western union',\n",
        "  'international money transfer'],\n",
        " ['investment',\n",
        "  'invest',\n",
        "  'stock',\n",
        "  'portfolio',\n",
        "  'gic',\n",
        "  'guaranteed certificate',\n",
        "  'rrsp',\n",
        "  'registered retirement saving plan',\n",
        "  'eft',\n",
        "  'interest rate',\n",
        "  'rate',\n",
        "  'fund',\n",
        "  'tfsa'],\n",
        " ['login',\n",
        "  'logout',\n",
        "  'log',\n",
        "  'logging',\n",
        "  'log account',\n",
        "  'sign in',\n",
        "  'sign out',\n",
        "  'log in',\n",
        "  'log out',\n",
        "  'password',\n",
        "  'username',\n",
        "  'logging'],\n",
        " ['quick balance',\n",
        "  'fast balance',\n",
        "  'balance view',\n",
        "  'balance',\n",
        "  'quick view'],\n",
        " ['new card',\n",
        "  'request card',\n",
        "  'request new card',\n",
        "  'replacement',\n",
        "  'replacement card',\n",
        "  'lost',\n",
        "  'stolen'],\n",
        " ['reward',\n",
        "  'points',\n",
        "  'loyalty',\n",
        "  'scene',\n",
        "  'scene point',\n",
        "  'point',\n",
        "  'shop',\n",
        "  'redeem',\n",
        "  'bonus'],\n",
        " ['save statement',\n",
        "  'share statement',\n",
        "  'statement',\n",
        "  'save',\n",
        "  'share',\n",
        "  'transaction',\n",
        "  'saving',\n",
        "  'sharing',\n",
        "  'payment',\n",
        "  'history']]"
      ],
      "metadata": {
        "id": "VYjkRpAiX_Mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idnFQD4DhYEA"
      },
      "outputs": [],
      "source": [
        "topic_model_3 = BERTopic(\n",
        "\n",
        "  # Pipeline models\n",
        "  umap_model=umap_model,\n",
        "  hdbscan_model=hdbscan_model,\n",
        "  vectorizer_model=vectorizer_model,\n",
        "\n",
        "  # Hyperparameters\n",
        "  #top_n_words=10,\n",
        "  #verbose=True\n",
        "\n",
        "  seed_topic_list=seed_topic_list_3\n",
        ")\n",
        "\n",
        "topics_3, probs_3 = topic_model_3.fit_transform(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1rjpJo-_FR9"
      },
      "outputs": [],
      "source": [
        "topic_model_3.get_topic_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ono3E08szkr"
      },
      "outputs": [],
      "source": [
        "# df_result = pd.read_csv('/content/bert_topics.csv')\n",
        "\n",
        "df_result = topic_model_3.get_topic_info()\n",
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjq4a2fOatg-"
      },
      "outputs": [],
      "source": [
        "topic_array = [-1] * len(df_result)\n",
        "topic_array_name = [''] * len(df_result)\n",
        "\n",
        "for j in range(len(df_result)):\n",
        "  topic_keyword_list = df_result.loc[j, 'Representation']\n",
        "  best_match = -1\n",
        "  max_matching = 0\n",
        "\n",
        "  for i in range(20):\n",
        "    cur_count = len(list(set(topic_keyword_list) & set(seed_topic_list_2[i])))\n",
        "\n",
        "    if cur_count > max_matching:\n",
        "        max_matching = cur_count\n",
        "        best_match = i\n",
        "  if max_matching >= 2:\n",
        "    topic_array[j] = best_match\n",
        "    topic_array_name[j] = topic.loc[best_match, \"Topics\"]\n",
        "\n",
        "\n",
        "df_result.loc[:, \"Topic_Category\"] = topic_array\n",
        "df_result.loc[:, \"Topic_Category Name\"] = topic_array_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDWRM9O-tUzb"
      },
      "outputs": [],
      "source": [
        "df_result"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_result.to_csv('/content/keyword_matching_result.csv')"
      ],
      "metadata": {
        "id": "nU_URjXaSqvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics_array =  ['2SV', 'Application Performance', 'Accessibility',\n",
        "       'Appointment_Booking', 'Biometric_Login', 'Budgeting', 'Chat',\n",
        "       'Cheque_Deposit', 'Credit_Score', 'Email_Money_Transfer', 'Errors',\n",
        "       'Fee', 'Info_Alerts', 'International_Money_Movement',\n",
        "       'Investments', 'Login_and_Logout_Issues', 'Quick_Balance',\n",
        "       'Request_New_Card', 'Rewards', 'Save_and_Share_Statements', 'Useless']"
      ],
      "metadata": {
        "id": "2gsGbtw_S1Fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_array = [0] * 21\n",
        "\n",
        "for i in range(len(df_result)):\n",
        "  if df_result.loc[i, \"Topic_Category\"] == -1:\n",
        "    count_array[20] += df_result.loc[i, \"Count\"]\n",
        "  else:\n",
        "    count_array[df_result.loc[i, \"Topic_Category\"]] += df_result.loc[i, \"Count\"]\n",
        "\n",
        "categorizing_df = pd.DataFrame(data= {'Topic': topics_array, 'Count': count_array})"
      ],
      "metadata": {
        "id": "tVO4OOY-S5sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorizing_df"
      ],
      "metadata": {
        "id": "VLkosAE3YPaA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorizing_df.to_csv('/content/Keyword_Categorizing.csv')"
      ],
      "metadata": {
        "id": "00m5ToJNS9Qv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}